{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10954746,"sourceType":"datasetVersion","datasetId":6814846}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom scipy.stats import norm\nfrom itertools import product\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nfrom copy import deepcopy\nimport time\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport optuna\n\n#import wandb\n#wandb.login(key='0057f517a0acb0992a4695e1d89cff691b8a8960')\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T17:49:36.496660Z","iopub.execute_input":"2025-03-07T17:49:36.496842Z","iopub.status.idle":"2025-03-07T17:49:41.324248Z","shell.execute_reply.started":"2025-03-07T17:49:36.496824Z","shell.execute_reply":"2025-03-07T17:49:41.323356Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"SABR_price = pd.read_csv('/kaggle/input/sabr-def1/SABR_model_DEF_PARTE01.csv')\nSABR_price[\"S0\"] = SABR_price[\"S0\"] * np.exp(SABR_price[\"r\"] * SABR_price[\"T\"])\nSABR_price.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T17:55:14.006210Z","iopub.execute_input":"2025-03-07T17:55:14.006515Z","iopub.status.idle":"2025-03-07T17:55:29.985496Z","shell.execute_reply.started":"2025-03-07T17:55:14.006496Z","shell.execute_reply":"2025-03-07T17:55:29.984589Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"          S0     alpha  beta       rho        nu         r    T   K        IV  \\\n0  85.495288  0.067447   1.0  0.520296  0.124547  0.024805  0.2  50  0.386060   \n1  85.495288  0.067447   1.0  0.520296  0.124547  0.024805  0.2  55  0.321857   \n2  85.495288  0.067447   1.0  0.520296  0.124547  0.024805  0.2  60  0.262809   \n3  85.495288  0.067447   1.0  0.520296  0.124547  0.024805  0.2  65  0.207931   \n4  85.495288  0.067447   1.0  0.520296  0.124547  0.024805  0.2  70  0.156389   \n\n   IV_Approx   BS Price  SABR Price  BS Price IV  \n0   0.060740  35.742719   35.745352    35.745352  \n1   0.060176  30.767462   30.770095    30.770095  \n2   0.060290  25.792205   25.794838    25.794838  \n3   0.060993  20.816948   20.819582    20.819582  \n4   0.062161  15.841691   15.844325    15.844325  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>S0</th>\n      <th>alpha</th>\n      <th>beta</th>\n      <th>rho</th>\n      <th>nu</th>\n      <th>r</th>\n      <th>T</th>\n      <th>K</th>\n      <th>IV</th>\n      <th>IV_Approx</th>\n      <th>BS Price</th>\n      <th>SABR Price</th>\n      <th>BS Price IV</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>85.495288</td>\n      <td>0.067447</td>\n      <td>1.0</td>\n      <td>0.520296</td>\n      <td>0.124547</td>\n      <td>0.024805</td>\n      <td>0.2</td>\n      <td>50</td>\n      <td>0.386060</td>\n      <td>0.060740</td>\n      <td>35.742719</td>\n      <td>35.745352</td>\n      <td>35.745352</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>85.495288</td>\n      <td>0.067447</td>\n      <td>1.0</td>\n      <td>0.520296</td>\n      <td>0.124547</td>\n      <td>0.024805</td>\n      <td>0.2</td>\n      <td>55</td>\n      <td>0.321857</td>\n      <td>0.060176</td>\n      <td>30.767462</td>\n      <td>30.770095</td>\n      <td>30.770095</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>85.495288</td>\n      <td>0.067447</td>\n      <td>1.0</td>\n      <td>0.520296</td>\n      <td>0.124547</td>\n      <td>0.024805</td>\n      <td>0.2</td>\n      <td>60</td>\n      <td>0.262809</td>\n      <td>0.060290</td>\n      <td>25.792205</td>\n      <td>25.794838</td>\n      <td>25.794838</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>85.495288</td>\n      <td>0.067447</td>\n      <td>1.0</td>\n      <td>0.520296</td>\n      <td>0.124547</td>\n      <td>0.024805</td>\n      <td>0.2</td>\n      <td>65</td>\n      <td>0.207931</td>\n      <td>0.060993</td>\n      <td>20.816948</td>\n      <td>20.819582</td>\n      <td>20.819582</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>85.495288</td>\n      <td>0.067447</td>\n      <td>1.0</td>\n      <td>0.520296</td>\n      <td>0.124547</td>\n      <td>0.024805</td>\n      <td>0.2</td>\n      <td>70</td>\n      <td>0.156389</td>\n      <td>0.062161</td>\n      <td>15.841691</td>\n      <td>15.844325</td>\n      <td>15.844325</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"class OptionPricingDatasetSABR(torch.utils.data.Dataset):\n    def __init__(self, X, y):\n        self.X = torch.tensor(X.values, dtype=torch.float32).to(device)\n        self.y = torch.tensor(y.values, dtype=torch.float32).to(device)\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T18:01:02.991393Z","iopub.execute_input":"2025-03-07T18:01:02.991686Z","iopub.status.idle":"2025-03-07T18:01:02.996584Z","shell.execute_reply.started":"2025-03-07T18:01:02.991665Z","shell.execute_reply":"2025-03-07T18:01:02.995550Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"X_sabr = SABR_price.drop(['BS Price', 'BS Price IV','IV','SABR Price','beta','IV_Approx'], axis=1)  # Features\ny_sabr = SABR_price['SABR Price']  # Target\n\nX_train_sabr , X_temp_sabr , y_train_sabr , y_temp_sabr  = train_test_split(X_sabr , y_sabr , test_size=0.2, random_state=42)         # Train/Test+Val\nX_val_sabr , X_test_sabr , y_val_sabr , y_test_sabr  = train_test_split(X_temp_sabr, y_temp_sabr, test_size=0.5, random_state=42)  # Val/Test\n\n\nscaler = MinMaxScaler()\nX_train_normalized_sabr  = scaler.fit_transform(X_train_sabr)  # Fit on training data and transform\nX_val_normalized_sabr  = scaler.transform(X_val_sabr)         # Transform validation data\nX_test_normalized_sabr  = scaler.transform(X_test_sabr)       # Transform test data\n\n# Convert normalized data back into DataFrames for DataLoader compatibility\nX_train_normalized_sabr  = pd.DataFrame(X_train_normalized_sabr , columns=X_sabr.columns)\nX_val_normalized_sabr  = pd.DataFrame(X_val_normalized_sabr , columns=X_sabr.columns)\nX_test_normalized_sabr  = pd.DataFrame(X_test_normalized_sabr , columns=X_sabr.columns)\n\n# Create PyTorch datasets\ntrain_dataset_sabr  = OptionPricingDatasetSABR(X_train_normalized_sabr , y_train_sabr)\nval_dataset_sabr  = OptionPricingDatasetSABR(X_val_normalized_sabr , y_val_sabr)\ntest_dataset_sabr  = OptionPricingDatasetSABR(X_test_normalized_sabr , y_test_sabr)\n\n# Create DataLoaders\n#train_loader_sabr  = torch.utils.data.DataLoader(train_dataset_sabr , batch_size=256, shuffle=True)\n#val_loader_sabr  = torch.utils.data.DataLoader(val_dataset_sabr , batch_size=256, shuffle=False)\n#test_loader_sabr  = torch.utils.data.DataLoader(test_dataset_sabr , batch_size=256, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T18:00:51.492142Z","iopub.execute_input":"2025-03-07T18:00:51.492463Z","iopub.status.idle":"2025-03-07T18:00:52.821141Z","shell.execute_reply.started":"2025-03-07T18:00:51.492442Z","shell.execute_reply":"2025-03-07T18:00:52.820419Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def train_model(model, train_loader, val_loader, mse_criterion, optimizer, device, num_epochs=50, patience=5):\n    best_loss = float('inf')\n    counter = 0\n    best_model = deepcopy(model.state_dict())\n    all_predictions = []\n    all_targets = []\n    for epoch in range(num_epochs):\n        model.train()\n        running_loss_mse = 0.0\n\n        for inputs, targets in tqdm(train_loader, desc=\"Training Epoch\", unit=\"batch\"):\n            inputs, targets = inputs.to(device), targets.to(device)\n\n            # Forward pass\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            targets = targets.unsqueeze(1)\n\n            # Compute MSE loss for monitoring (not used in training)\n            loss_mse = mse_criterion(outputs, targets)\n\n            # Backward pass and optimization\n            loss_mse.backward()\n            optimizer.step()\n\n            running_loss_mse += loss_mse.item()\n\n            # Collect predictions and true values for MAE & R² computation\n            all_predictions.extend(outputs.cpu().detach().numpy().flatten())\n            all_targets.extend(targets.cpu().detach().numpy().flatten())\n            \n        avg_train_loss_mse = running_loss_mse / X_train_sabr.shape[0]\n        avg_train_loss_mae = mean_absolute_error(all_targets, all_predictions)\n        train_r2_score = r2_score(all_targets, all_predictions)\n\n        print(f\"Epoch {epoch + 1}/{num_epochs} - \"\n              f\"Train MSE: {avg_train_loss_mse:.10f}, \"\n              f\"Train MAE: {avg_train_loss_mae:.10f}, \"\n              f\"Train R²: {train_r2_score:.10f}\")\n\n        #print(f\"Epoch {epoch + 1}/{num_epochs},  Train MSE Loss: {avg_train_loss_mse:.10f}\")\n\n        # Validation after each epoch\n        avg_val_loss_mse = validation_model(model, val_loader, mse_criterion, device)\n\n\n        # Early stopping based on custom loss\n        if avg_val_loss_mse < best_loss:\n            best_loss = avg_val_loss_mse\n            counter = 0\n            best_model = deepcopy(model.state_dict())  # Save the best model state\n        else:\n            counter += 1\n\n        if counter >= patience:\n            print(f\"Early stopping triggered at epoch {epoch + 1}\")\n            break\n\n    model.load_state_dict(best_model)  # Restore the best model\n    return model, avg_train_loss_mse, avg_val_loss_mse, best_loss\n\n# Validation function\ndef validation_model(model, val_loader,  mse_criterion, device):\n    model.eval()\n    val_loss_mse = 0.0\n    all_predictions = []\n    all_targets = []\n    with torch.no_grad():\n        for inputs, targets in tqdm(val_loader, desc=\"Validation\", unit=\"batch\"):\n            inputs, targets = inputs.to(device), targets.to(device)\n\n            # Forward pass\n            outputs = model(inputs)\n            targets = targets.unsqueeze(1)\n\n            loss_mse = mse_criterion(outputs, targets)        # MSE loss\n\n            val_loss_mse += loss_mse.item()\n            \n            # Collect predictions and true values for MAE & R² computation\n            all_predictions.extend(outputs.cpu().detach().numpy().flatten())\n            all_targets.extend(targets.cpu().detach().numpy().flatten())\n            \n        avg_val_loss_mse = val_loss_mse / X_val_sabr.shape[0]\n        avg_val_loss_mae = mean_absolute_error(all_targets, all_predictions)\n        val_r2_score = r2_score(all_targets, all_predictions)\n\n        print(f\"Val MSE: {avg_val_loss_mse:.10f}, \"\n              f\"Val  MAE: {avg_val_loss_mae:.10f}, \"\n              f\"Val  R²: {val_r2_score:.10f}\")\n    #print(f\" Validation MSE Loss: {avg_val_loss_mse:.10f}\")\n    return  avg_val_loss_mse\n\n# Testing function\ndef test_model(model, test_loader, mse_criterion, device):\n    model.eval()\n    \n    test_loss_mse = 0.0\n    all_predictions = []\n    all_targets = []\n\n    with torch.no_grad():\n        for inputs, targets in tqdm(test_loader, desc=\"Testing\", unit=\"batch\"):\n            inputs, targets = inputs.to(device), targets.to(device)\n\n            # Forward pass\n            outputs = model(inputs)\n            targets = targets.unsqueeze(1)\n\n            loss_mse = mse_criterion(outputs, targets)        # MSE loss\n\n            test_loss_mse += loss_mse.item()\n\n            all_predictions.extend(outputs.cpu().numpy())\n            all_targets.extend(targets.cpu().numpy())\n\n    avg_test_loss_mse = test_loss_mse / X_test_sabr.shape[0]\n    avg_test_loss_mae = mean_absolute_error(all_targets, all_predictions)\n    test_r2_score = r2_score(all_targets, all_predictions)\n\n    print(f\"Test MSE: {avg_test_loss_mse:.10f}, \"\n          f\"Test MAE: {avg_test_loss_mae:.10f}, \"\n          f\"Test R²: {test_r2_score:.10f}\")\n\n    return  avg_test_loss_mse, all_predictions, all_targets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T18:10:11.045525Z","iopub.execute_input":"2025-03-07T18:10:11.045862Z","iopub.status.idle":"2025-03-07T18:10:11.058772Z","shell.execute_reply.started":"2025-03-07T18:10:11.045837Z","shell.execute_reply":"2025-03-07T18:10:11.057961Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"def objective(trial):\n    # Define hyperparameters to optimize\n    num_layers = trial.suggest_int(\"num_layers\", 2, 5)  # Number of hidden layers\n    hidden_size = trial.suggest_int(\"hidden_size\", 50, 500)  # Number of neurons per layer\n    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.0, 0.2)  # Dropout rate\n    learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 1e-3, log=True)  # Learning rate\n    batch_size = trial.suggest_categorical(\"batch_size\", [32,64,128,256])  # Batch size\n\n    # Define the MLP model dynamically\n    class MLP(nn.Module):\n        def __init__(self, input_dim, num_layers, hidden_size, dropout_rate):\n            super(MLP, self).__init__()\n            layers = [nn.Linear(input_dim, hidden_size), nn.ReLU(), nn.Dropout(dropout_rate)]\n            for _ in range(num_layers - 2):\n                layers.extend([nn.Linear(hidden_size, hidden_size), nn.ReLU(), nn.Dropout(dropout_rate)])\n            layers.extend([nn.Linear(hidden_size, int(hidden_size/4)), nn.ReLU(), nn.Dropout(dropout_rate)])\n            layers.extend([nn.Linear(int(hidden_size/4), int(hidden_size/4)), nn.ReLU(), nn.Dropout(dropout_rate)])    \n            layers.append(nn.Linear(int(hidden_size/4), 1))  # Output layer\n            self.model = nn.Sequential(*layers)\n\n        def forward(self, x):\n            return self.model(x)\n\n    # Initialize model\n    input_dim = X_train_sabr.shape[1]\n    model = MLP(input_dim, num_layers, hidden_size, dropout_rate).to(device)\n\n    # Define loss function and optimizer\n    mse_criterion = nn.MSELoss()\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n    # Create DataLoaders with the suggested batch_size\n    train_loader_sabr = torch.utils.data.DataLoader(train_dataset_sabr, batch_size=batch_size, shuffle=True)\n    val_loader_sabr = torch.utils.data.DataLoader(val_dataset_sabr, batch_size=batch_size, shuffle=False)\n\n    # Train the model\n    trained_model, _, _, best_val_loss = train_model(\n        model, train_loader_sabr, val_loader_sabr, mse_criterion, optimizer, device, num_epochs=50, patience=5\n    )\n\n    return best_val_loss  # Minimize validation loss\n\n# Run Optuna study with the updated objective function\nstudy = optuna.create_study(study_name=\"MLP_Optimization\", direction=\"minimize\", sampler=optuna.samplers.TPESampler())\nstudy.optimize(objective, n_trials=1)  # Try 30 different configurations\n\n# Print best hyperparameters\nprint(\"Best Hyperparameters:\", study.best_params)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T18:10:16.595420Z","iopub.execute_input":"2025-03-07T18:10:16.595762Z","iopub.status.idle":"2025-03-07T18:25:43.507383Z","shell.execute_reply.started":"2025-03-07T18:10:16.595734Z","shell.execute_reply":"2025-03-07T18:25:43.506493Z"}},"outputs":[{"name":"stderr","text":"[I 2025-03-07 18:10:16,601] A new study created in memory with name: MLP_Optimization\nTraining Epoch: 100%|██████████| 48765/48765 [02:23<00:00, 339.34batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/50 - Train MSE: 0.0496824509, Train MAE: 0.5428456068, Train R²: 0.9906111557\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 6096/6096 [00:06<00:00, 934.89batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Val MSE: 0.0006512187, Val  MAE: 0.1427313536, Val  R²: 0.9998766053\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch: 100%|██████████| 48765/48765 [02:24<00:00, 336.61batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/50 - Train MSE: 0.0041191245, Train MAE: 0.4283646941, Train R²: 0.9949163694\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 6096/6096 [00:06<00:00, 936.70batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Val MSE: 0.0009226934, Val  MAE: 0.1710327268, Val  R²: 0.9998251670\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch: 100%|██████████| 48765/48765 [02:25<00:00, 335.40batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/50 - Train MSE: 0.0033354486, Train MAE: 0.3781129420, Train R²: 0.9964008055\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 6096/6096 [00:06<00:00, 933.34batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Val MSE: 0.0037073947, Val  MAE: 0.3191560805, Val  R²: 0.9992975183\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch: 100%|██████████| 48765/48765 [02:25<00:00, 335.85batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/50 - Train MSE: 0.0030262765, Train MAE: 0.3487455547, Train R²: 0.9971576303\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 6096/6096 [00:06<00:00, 917.92batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Val MSE: 0.0037629991, Val  MAE: 0.3222465217, Val  R²: 0.9992869927\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch: 100%|██████████| 48765/48765 [02:25<00:00, 336.19batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/50 - Train MSE: 0.0028496604, Train MAE: 0.3289677501, Train R²: 0.9976184006\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 6096/6096 [00:06<00:00, 924.16batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Val MSE: 0.0035819344, Val  MAE: 0.3181206286, Val  R²: 0.9993212933\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch: 100%|██████████| 48765/48765 [02:24<00:00, 336.50batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/50 - Train MSE: 0.0027030204, Train MAE: 0.3143320084, Train R²: 0.9979301993\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 6096/6096 [00:06<00:00, 919.47batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Val MSE: 0.0063514960, Val  MAE: 0.4427323043, Val  R²: 0.9987965151\nEarly stopping triggered at epoch 6\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-03-07 18:25:43,502] Trial 0 finished with value: 0.0006512187028031105 and parameters: {'num_layers': 4, 'hidden_size': 225, 'dropout_rate': 0.006211959466474126, 'learning_rate': 0.00011768237318338051, 'batch_size': 64}. Best is trial 0 with value: 0.0006512187028031105.\n","output_type":"stream"},{"name":"stdout","text":"Best Hyperparameters: {'num_layers': 4, 'hidden_size': 225, 'dropout_rate': 0.006211959466474126, 'learning_rate': 0.00011768237318338051, 'batch_size': 64}\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"class MLP(nn.Module):\n    def __init__(self, input_dim):\n        super(MLP, self).__init__()\n        self.model = nn.Sequential(\n            nn.Linear(input_dim, 200),  # Input layer\n            nn.ReLU(),\n            nn.Linear(200, 200),        # Hidden layer\n            nn.ReLU(),\n            nn.Linear(200, 200),         # Hidden layer\n            nn.ReLU(),\n            nn.Linear(200, 50),          # Hidden layer\n            nn.ReLU(),\n            nn.Linear(50, 1),          # Hidden layer (Matches checkpoint)\n        )\n    def forward(self, x):\n        return self.model(x)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training phase\ntrained_model, train_losses, val_loss, best_loss = train_model(\n    model, train_loader_sabr, val_loader_sabr, mse_criterion, optimizer, device, num_epochs=50, patience=5\n)\n\n# Test phase\ntest_loss, predictions, true_values = test_model(trained_model, test_loader_sabr, mse_criterion, device)\n\n# Save model\ntorch.save(trained_model.state_dict(), \"mlp_SABR.pth\")\nprint(\"Model saved!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
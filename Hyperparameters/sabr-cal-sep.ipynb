{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10968143,"sourceType":"datasetVersion","datasetId":6824300},{"sourceId":11175585,"sourceType":"datasetVersion","datasetId":6974976},{"sourceId":11184116,"sourceType":"datasetVersion","datasetId":6981313}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom scipy.stats import norm\nfrom itertools import product\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nfrom copy import deepcopy\nimport time\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport optuna\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T10:08:17.016593Z","iopub.execute_input":"2025-03-27T10:08:17.017150Z","iopub.status.idle":"2025-03-27T10:08:23.387567Z","shell.execute_reply.started":"2025-03-27T10:08:17.017101Z","shell.execute_reply":"2025-03-27T10:08:23.386257Z"}},"outputs":[{"name":"stdout","text":"Using device: cpu\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"SABR_cal = pd.read_csv('/kaggle/input/sabr-cal-30/SABR_model_results.csv')\nSABR_cal.head()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-26T17:08:14.042987Z","iopub.execute_input":"2025-03-26T17:08:14.043363Z","iopub.status.idle":"2025-03-26T17:13:16.594290Z","shell.execute_reply.started":"2025-03-26T17:08:14.043335Z","shell.execute_reply":"2025-03-26T17:13:16.593042Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"         S_1        S_2        S_3        S_4        S_5        S_6  \\\n0  90.422466  90.805575  91.447185  91.613698  92.952651  94.127607   \n1  90.422466  91.833751  92.266471  90.639588  89.925484  89.338754   \n2  90.422466  90.647801  89.949748  89.232632  87.787124  88.314657   \n3  90.422466  90.013999  90.229666  93.225704  94.396989  94.202796   \n4  90.422466  92.345150  91.183986  88.825354  90.778707  91.732969   \n\n         S_7        S_8        S_9       S_10  ...  sigma_351  sigma_352  \\\n0  94.297985  93.500430  95.717052  97.809856  ...   0.245079   0.243950   \n1  91.262856  90.850083  89.777434  89.554650  ...   0.261715   0.261511   \n2  88.183894  86.932109  86.299791  83.874874  ...   0.238487   0.237984   \n3  92.748704  92.821197  92.352620  92.208894  ...   0.221166   0.222519   \n4  93.124575  92.806635  90.812415  87.677114  ...   0.252428   0.250448   \n\n   sigma_353         S0     alpha  beta       rho        nu         r    T  \n0   0.244192  88.448489  0.242849   1.0  0.203859  0.063728  0.015766  1.4  \n1   0.261084  88.448489  0.242849   1.0  0.203859  0.063728  0.015766  1.4  \n2   0.237329  88.448489  0.242849   1.0  0.203859  0.063728  0.015766  1.4  \n3   0.224034  88.448489  0.242849   1.0  0.203859  0.063728  0.015766  1.4  \n4   0.251429  88.448489  0.242849   1.0  0.203859  0.063728  0.015766  1.4  \n\n[5 rows x 713 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>S_1</th>\n      <th>S_2</th>\n      <th>S_3</th>\n      <th>S_4</th>\n      <th>S_5</th>\n      <th>S_6</th>\n      <th>S_7</th>\n      <th>S_8</th>\n      <th>S_9</th>\n      <th>S_10</th>\n      <th>...</th>\n      <th>sigma_351</th>\n      <th>sigma_352</th>\n      <th>sigma_353</th>\n      <th>S0</th>\n      <th>alpha</th>\n      <th>beta</th>\n      <th>rho</th>\n      <th>nu</th>\n      <th>r</th>\n      <th>T</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>90.422466</td>\n      <td>90.805575</td>\n      <td>91.447185</td>\n      <td>91.613698</td>\n      <td>92.952651</td>\n      <td>94.127607</td>\n      <td>94.297985</td>\n      <td>93.500430</td>\n      <td>95.717052</td>\n      <td>97.809856</td>\n      <td>...</td>\n      <td>0.245079</td>\n      <td>0.243950</td>\n      <td>0.244192</td>\n      <td>88.448489</td>\n      <td>0.242849</td>\n      <td>1.0</td>\n      <td>0.203859</td>\n      <td>0.063728</td>\n      <td>0.015766</td>\n      <td>1.4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>90.422466</td>\n      <td>91.833751</td>\n      <td>92.266471</td>\n      <td>90.639588</td>\n      <td>89.925484</td>\n      <td>89.338754</td>\n      <td>91.262856</td>\n      <td>90.850083</td>\n      <td>89.777434</td>\n      <td>89.554650</td>\n      <td>...</td>\n      <td>0.261715</td>\n      <td>0.261511</td>\n      <td>0.261084</td>\n      <td>88.448489</td>\n      <td>0.242849</td>\n      <td>1.0</td>\n      <td>0.203859</td>\n      <td>0.063728</td>\n      <td>0.015766</td>\n      <td>1.4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>90.422466</td>\n      <td>90.647801</td>\n      <td>89.949748</td>\n      <td>89.232632</td>\n      <td>87.787124</td>\n      <td>88.314657</td>\n      <td>88.183894</td>\n      <td>86.932109</td>\n      <td>86.299791</td>\n      <td>83.874874</td>\n      <td>...</td>\n      <td>0.238487</td>\n      <td>0.237984</td>\n      <td>0.237329</td>\n      <td>88.448489</td>\n      <td>0.242849</td>\n      <td>1.0</td>\n      <td>0.203859</td>\n      <td>0.063728</td>\n      <td>0.015766</td>\n      <td>1.4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>90.422466</td>\n      <td>90.013999</td>\n      <td>90.229666</td>\n      <td>93.225704</td>\n      <td>94.396989</td>\n      <td>94.202796</td>\n      <td>92.748704</td>\n      <td>92.821197</td>\n      <td>92.352620</td>\n      <td>92.208894</td>\n      <td>...</td>\n      <td>0.221166</td>\n      <td>0.222519</td>\n      <td>0.224034</td>\n      <td>88.448489</td>\n      <td>0.242849</td>\n      <td>1.0</td>\n      <td>0.203859</td>\n      <td>0.063728</td>\n      <td>0.015766</td>\n      <td>1.4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>90.422466</td>\n      <td>92.345150</td>\n      <td>91.183986</td>\n      <td>88.825354</td>\n      <td>90.778707</td>\n      <td>91.732969</td>\n      <td>93.124575</td>\n      <td>92.806635</td>\n      <td>90.812415</td>\n      <td>87.677114</td>\n      <td>...</td>\n      <td>0.252428</td>\n      <td>0.250448</td>\n      <td>0.251429</td>\n      <td>88.448489</td>\n      <td>0.242849</td>\n      <td>1.0</td>\n      <td>0.203859</td>\n      <td>0.063728</td>\n      <td>0.015766</td>\n      <td>1.4</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 713 columns</p>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"class OptionPricingDatasetSABR(torch.utils.data.Dataset):\n    def __init__(self, X, y):\n        self.X = torch.tensor(X.values, dtype=torch.float32).to(device)\n        self.y = torch.tensor(y.values, dtype=torch.float32).to(device)\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T17:15:27.990264Z","iopub.execute_input":"2025-03-26T17:15:27.990836Z","iopub.status.idle":"2025-03-26T17:15:27.997664Z","shell.execute_reply.started":"2025-03-26T17:15:27.990796Z","shell.execute_reply":"2025-03-26T17:15:27.996471Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"X_sabr_cal = SABR_cal.drop(['alpha','beta','rho','nu'], axis=1)  # Features\ny_sabr_cal = SABR_cal[['alpha','rho','nu']]          # Target\n\nX_train_sabr_cal , X_temp_sabr_cal , y_train_sabr_cal , y_temp_sabr_cal  = train_test_split(X_sabr_cal , y_sabr_cal , test_size=0.2, random_state=42)         # Train/Test+Val\nX_val_sabr_cal , X_test_sabr_cal , y_val_sabr_cal , y_test_sabr_cal  = train_test_split(X_temp_sabr_cal, y_temp_sabr_cal, test_size=0.5, random_state=42)  # Val/Test\n\n\nscaler = MinMaxScaler()\nX_train_normalized_sabr_cal  = scaler.fit_transform(X_train_sabr_cal)  # Fit on training data and transform\nX_val_normalized_sabr_cal  = scaler.transform(X_val_sabr_cal)         # Transform validation data\nX_test_normalized_sabr_cal  = scaler.transform(X_test_sabr_cal)       # Transform test data\n\n# Convert normalized data back into DataFrames for DataLoader compatibility\nX_train_normalized_sabr_cal  = pd.DataFrame(X_train_normalized_sabr_cal , columns=X_sabr_cal.columns)\nX_val_normalized_sabr_cal  = pd.DataFrame(X_val_normalized_sabr_cal , columns=X_sabr_cal.columns)\nX_test_normalized_sabr_cal  = pd.DataFrame(X_test_normalized_sabr_cal , columns=X_sabr_cal.columns)\n\n# Create PyTorch datasets\ntrain_dataset_sabr_cal  = OptionPricingDatasetSABR(X_train_normalized_sabr_cal , y_train_sabr_cal)\nval_dataset_sabr_cal  = OptionPricingDatasetSABR(X_val_normalized_sabr_cal , y_val_sabr_cal)\ntest_dataset_sabr_cal  = OptionPricingDatasetSABR(X_test_normalized_sabr_cal , y_test_sabr_cal)\n\n# Create DataLoaders\ntrain_loader_sabr_cal  = torch.utils.data.DataLoader(train_dataset_sabr_cal , batch_size=1024, shuffle=True)\nval_loader_sabr_cal  = torch.utils.data.DataLoader(val_dataset_sabr_cal , batch_size=1024, shuffle=False)\ntest_loader_sabr_cal  = torch.utils.data.DataLoader(test_dataset_sabr_cal , batch_size=1024, shuffle=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T17:15:29.517830Z","iopub.execute_input":"2025-03-26T17:15:29.518357Z","iopub.status.idle":"2025-03-26T17:15:58.636176Z","shell.execute_reply.started":"2025-03-26T17:15:29.518313Z","shell.execute_reply":"2025-03-26T17:15:58.635226Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def train_model_sabr_cal(model, train_loader, val_loader, mse_criterion, optimizer, device, num_epochs=50, patience=5):\n    best_loss = float('inf')\n    counter = 0\n    best_model = deepcopy(model.state_dict())\n    all_predictions = []\n    all_targets = []\n    for epoch in range(num_epochs):\n        model.train()\n        running_loss_mse = 0.0\n\n        for inputs, targets in tqdm(train_loader, desc=\"Training Epoch\", unit=\"batch\"):\n            inputs, targets = inputs.to(device), targets.to(device)\n\n            # Forward pass\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            targets = targets.squeeze(1)\n\n            # Compute MSE loss for monitoring (not used in training)\n            loss_mse = mse_criterion(outputs, targets)\n\n            # Backward pass and optimization\n            loss_mse.backward()\n            optimizer.step()\n\n            running_loss_mse += loss_mse.item()\n            \n        avg_train_loss_mse = running_loss_mse / len(train_loader)\n        print(f\"Epoch {epoch + 1}/{num_epochs} - \"\n              f\"Train MSE: {avg_train_loss_mse:.10f}\")\n\n        #print(f\"Epoch {epoch + 1}/{num_epochs},  Train MSE Loss: {avg_train_loss_mse:.10f}\")\n\n        # Validation after each epoch\n        avg_val_loss_mse = validation_model_sabr_cal(model, val_loader, mse_criterion, device)\n\n\n        # Early stopping based on custom loss\n        if avg_val_loss_mse < best_loss:\n            best_loss = avg_val_loss_mse\n            counter = 0\n            best_model = deepcopy(model.state_dict())  # Save the best model state\n        else:\n            counter += 1\n\n        if counter >= patience:\n            print(f\"Early stopping triggered at epoch {epoch + 1}\")\n            break\n\n    model.load_state_dict(best_model)  # Restore the best model\n    return model, avg_train_loss_mse, avg_val_loss_mse, best_loss\n\n# Validation function\ndef validation_model_sabr_cal(model, val_loader,  mse_criterion, device):\n    model.eval()\n    val_loss_mse = 0.0\n    #all_predictions = []\n    #all_targets = []\n    with torch.no_grad():\n        for inputs, targets in tqdm(val_loader, desc=\"Validation\", unit=\"batch\"):\n            inputs, targets = inputs.to(device), targets.to(device)\n\n            # Forward pass\n            outputs = model(inputs)\n            targets = targets.squeeze(1)\n\n            loss_mse = mse_criterion(outputs, targets)        # MSE loss\n\n            val_loss_mse += loss_mse.item()\n\n            # Collect predictions and true values for MAE & R² computation\n            #all_predictions.extend(outputs.cpu().detach().numpy().flatten())\n            #all_targets.extend(targets.cpu().detach().numpy().flatten())\n            \n        avg_val_loss_mse = val_loss_mse / len(val_loader)\n        #avg_val_loss_mae = mean_absolute_error(all_targets, all_predictions)\n        #val_r2_score = r2_score(all_targets, all_predictions)\n\n        print(f\"Val MSE: {avg_val_loss_mse:.10f}\")#, \"\n              #f\"Val  MAE: {avg_val_loss_mae:.10f}, \"\n              #f\"Val  R²: {val_r2_score:.10f}\")\n    #print(f\" Validation MSE Loss: {avg_val_loss_mse:.10f}\")\n    return  avg_val_loss_mse\n\n# Testing function\ndef test_model_sabr_cal(model, test_loader, mse_criterion, device):\n    model.eval()\n    \n    test_loss_mse = 0.0\n    all_predictions = []\n    all_targets = []\n\n    with torch.no_grad():\n        for inputs, targets in tqdm(test_loader, desc=\"Testing\", unit=\"batch\"):\n            inputs, targets = inputs.to(device), targets.to(device)\n\n            # Forward pass\n            outputs = model(inputs)\n            targets = targets.squeeze(1)\n            \n            loss_mse = mse_criterion(outputs, targets)        # MSE loss\n\n            test_loss_mse += loss_mse.item()\n\n            all_predictions.extend(outputs.cpu().numpy())\n            all_targets.extend(targets.cpu().numpy())\n\n    avg_test_loss_mse= mean_squared_error(all_targets,all_predictions)\n    avg_test_loss_mae = mean_absolute_error(all_targets, all_predictions)\n    test_r2_score = r2_score(all_targets, all_predictions)\n\n    print(f\"Test MSE: {avg_test_loss_mse:.10f}, \"\n          f\"Test MAE: {avg_test_loss_mae:.10f}, \"\n          f\"Test R²: {test_r2_score:.10f}\")\n\n    return  avg_test_loss_mse, all_predictions, all_targets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T17:19:51.767844Z","iopub.execute_input":"2025-03-26T17:19:51.768220Z","iopub.status.idle":"2025-03-26T17:19:51.782770Z","shell.execute_reply.started":"2025-03-26T17:19:51.768171Z","shell.execute_reply":"2025-03-26T17:19:51.781716Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def objective(trial):\n    # Define hyperparameters\n    num_layers = trial.suggest_int(\"num_layers\", 2, 5)  # Number of hidden layers\n    hidden_size = trial.suggest_int(\"hidden_size\", 500, 2000)  # Neurons per layer\n    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.0, 0.2)  # Dropout rate\n    learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 1e-3, log=True)  # Learning rate\n    batch_size = trial.suggest_categorical(\"batch_size\", [256,512,1024,2048])  # Batch size\n\n    # Define the MLP model dynamically\n    class MLP(nn.Module):\n        def __init__(self, input_dim, num_layers, hidden_size, dropout_rate):\n            super(MLP, self).__init__()\n            layers = [nn.Linear(input_dim, hidden_size), nn.ReLU(), nn.Dropout(dropout_rate)]\n            for _ in range(num_layers - 1):\n                layers.extend([nn.Linear(hidden_size, hidden_size), nn.ReLU(), nn.Dropout(dropout_rate)])\n            layers.extend([nn.Linear(hidden_size, int(hidden_size/4)), nn.ReLU(), nn.Dropout(dropout_rate)])\n            layers.extend([nn.Linear(int(hidden_size/4), int(hidden_size/4)), nn.ReLU(), nn.Dropout(dropout_rate)])\n            layers.append(nn.Linear(int(hidden_size/4), 3))  # Output layer\n            self.model = nn.Sequential(*layers)\n\n        def forward(self, x):\n            return self.model(x)\n\n    # Initialize model\n    input_dim = X_train_sabr_cal.shape[1]\n    model = MLP(input_dim, num_layers, hidden_size, dropout_rate).to(device)\n\n    # Define loss function and optimizer\n    mse_criterion = nn.MSELoss()\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n    # Create DataLoaders with the suggested batch_size\n    train_loader_sabr_cal = torch.utils.data.DataLoader(train_dataset_sabr_cal, batch_size=batch_size, shuffle=True)\n    val_loader_sabr_cal = torch.utils.data.DataLoader(val_dataset_sabr_cal, batch_size=batch_size, shuffle=False)\n\n    # Train the model\n    trained_model, _, _, best_val_loss = train_model_sabr_cal(\n        model, train_loader_sabr_cal, val_loader_sabr_cal, mse_criterion, optimizer, device, num_epochs=50, patience=5\n    )\n    torch.save(trained_model.state_dict(), \"mlp_SABR_cal.pth\")\n    print(\"Model saved!\")\n    # Return validation loss (Optuna minimizes this)\n    return best_val_loss  \n\n# Use an SQLite database to store trials\nstorage = \"sqlite:///optuna_study.db\"\n\n# Run Optuna study with TPESampler and persistent storage\nstudy = optuna.create_study(\n    study_name=\"MLP_Optimization_vol\", \n    direction=\"minimize\", \n    sampler=optuna.samplers.TPESampler(), \n    storage=storage, \n    load_if_exists=True  # Resume if the study exists\n)\n\nstudy.optimize(objective, n_trials=20)  # Run 30 trials\n\n# Print best hyperparameters\nprint(\"Best Hyperparameters:\", study.best_params)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T17:19:54.658118Z","iopub.execute_input":"2025-03-26T17:19:54.658533Z","execution_failed":"2025-03-27T05:06:37.745Z"}},"outputs":[{"name":"stderr","text":"[I 2025-03-26 17:19:54,712] Using an existing study with name 'MLP_Optimization_vol' instead of creating a new one.\nTraining Epoch: 100%|██████████| 782/782 [02:16<00:00,  5.71batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/50 - Train MSE: 0.0606068775\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 98/98 [00:05<00:00, 18.61batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Val MSE: 0.0332750755\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch: 100%|██████████| 782/782 [02:27<00:00,  5.31batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/50 - Train MSE: 0.0317276151\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 98/98 [00:05<00:00, 19.09batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Val MSE: 0.0163428509\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch: 100%|██████████| 782/782 [02:26<00:00,  5.34batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/50 - Train MSE: 0.0202295324\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 98/98 [00:05<00:00, 19.28batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Val MSE: 0.0081340902\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch: 100%|██████████| 782/782 [02:23<00:00,  5.45batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/50 - Train MSE: 0.0143716644\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 98/98 [00:05<00:00, 18.64batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Val MSE: 0.0088383290\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch: 100%|██████████| 782/782 [02:23<00:00,  5.44batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/50 - Train MSE: 0.0115475465\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 98/98 [00:05<00:00, 19.40batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Val MSE: 0.0058866909\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch: 100%|██████████| 782/782 [02:25<00:00,  5.38batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/50 - Train MSE: 0.0101183542\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 98/98 [00:14<00:00,  6.78batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Val MSE: 0.0091796947\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch: 100%|██████████| 782/782 [02:25<00:00,  5.37batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/50 - Train MSE: 0.0078883309\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 98/98 [00:05<00:00, 17.49batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Val MSE: 0.0078348480\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch: 100%|██████████| 782/782 [02:25<00:00,  5.39batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/50 - Train MSE: 0.0084052787\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 98/98 [00:05<00:00, 19.16batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Val MSE: 0.0088391723\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch: 100%|██████████| 782/782 [02:26<00:00,  5.34batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9/50 - Train MSE: 0.0087232631\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 98/98 [00:05<00:00, 19.09batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Val MSE: 0.0163337889\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch: 100%|██████████| 782/782 [02:27<00:00,  5.31batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/50 - Train MSE: 0.0079852201\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 98/98 [00:05<00:00, 19.48batch/s]\n[I 2025-03-26 17:45:04,084] Trial 3 finished with value: 0.0058866909191924695 and parameters: {'num_layers': 2, 'hidden_size': 1021, 'dropout_rate': 0.19712927077787873, 'learning_rate': 0.0005317541785270053, 'batch_size': 1024}. Best is trial 3 with value: 0.0058866909191924695.\n","output_type":"stream"},{"name":"stdout","text":"Val MSE: 0.0152028957\nEarly stopping triggered at epoch 10\nModel saved!\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch: 100%|██████████| 3125/3125 [10:14<00:00,  5.08batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/50 - Train MSE: 0.0428222558\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 391/391 [00:15<00:00, 25.31batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Val MSE: 0.0147487464\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch: 100%|██████████| 3125/3125 [10:52<00:00,  4.79batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/50 - Train MSE: 0.0137985974\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 391/391 [00:16<00:00, 23.75batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Val MSE: 0.0046358439\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch: 100%|██████████| 3125/3125 [10:54<00:00,  4.77batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/50 - Train MSE: 0.0093095863\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 391/391 [00:15<00:00, 24.95batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Val MSE: 0.0036225612\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch:  90%|█████████ | 2823/3125 [09:52<01:01,  4.89batch/s]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# Test phase\ntest_loss, predictions, true_values = test_model_sabr_cal(trained_model, test_loader_sabr_cal, mse_criterion, device)\n\n# Save model\ntorch.save(trained_model.state_dict(), \"mlp_SABR_cal_rho.pth\")\nprint(\"Model saved!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Test phase\ntest_loss, predictions, true_values = test_model_sabr_cal(trained_model, test_loader_sabr_cal, mse_criterion, device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T10:58:10.747320Z","iopub.execute_input":"2025-03-18T10:58:10.747764Z","iopub.status.idle":"2025-03-18T10:58:10.773402Z","shell.execute_reply.started":"2025-03-18T10:58:10.747732Z","shell.execute_reply":"2025-03-18T10:58:10.771458Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-190906079d67>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Test phase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_model_sabr_cal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrained_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader_sabr_cal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmse_criterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'trained_model' is not defined"],"ename":"NameError","evalue":"name 'trained_model' is not defined","output_type":"error"}],"execution_count":11},{"cell_type":"code","source":"#print(test_loss)\nfor i in range(len(predictions)):\n    if i==30:\n        break\n    print(f\"Prediction: {predictions[i]}, True Value: {true_values[i]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T18:44:31.355520Z","iopub.execute_input":"2025-03-14T18:44:31.355871Z","iopub.status.idle":"2025-03-14T18:44:31.374962Z","shell.execute_reply.started":"2025-03-14T18:44:31.355840Z","shell.execute_reply":"2025-03-14T18:44:31.373512Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-37-8bb49a1bcaea>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#print(test_loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Prediction: {predictions[i]}, True Value: {true_values[i]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'predictions' is not defined"],"ename":"NameError","evalue":"name 'predictions' is not defined","output_type":"error"}],"execution_count":37},{"cell_type":"code","source":"db_path = \"/kaggle/input/dboptuna/optuna_study (2).db\"  # Replace with your actual path\nstudy_name = \"MLP_Optimization_vol\"  # Your study name\n\nstudy = optuna.load_study(study_name=study_name, storage=f\"sqlite:///{db_path}\")\n\n# Print the best trial\nprint(\"Best Trial:\")\nprint(f\"  Number: {study.best_trial.number}\")\nprint(f\"  Value: {study.best_trial.value}\")\nprint(f\"  Params: {study.best_trial.params}\")\n\n# Print all trials (optional)\nfor trial in study.trials:\n    print(f\"Trial {trial.number}: Value={trial.value}, Params={trial.params}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T10:10:51.962251Z","iopub.execute_input":"2025-03-27T10:10:51.963039Z","iopub.status.idle":"2025-03-27T10:10:54.315973Z","shell.execute_reply.started":"2025-03-27T10:10:51.962999Z","shell.execute_reply":"2025-03-27T10:10:54.314371Z"}},"outputs":[{"name":"stdout","text":"Best Trial:\n  Number: 7\n  Value: 0.0002476581408997418\n  Params: {'num_layers': 5, 'hidden_size': 770, 'dropout_rate': 0.058418252474786825, 'learning_rate': 0.00017537517229441845, 'batch_size': 1024}\nTrial 0: Value=None, Params={'num_layers': 4, 'hidden_size': 1309, 'dropout_rate': 0.18553568495516168, 'learning_rate': 0.00030904963939730847, 'batch_size': 256}\nTrial 1: Value=None, Params={'num_layers': 4, 'hidden_size': 1272, 'dropout_rate': 0.05510190161935529, 'learning_rate': 0.0002711541592576603, 'batch_size': 2048}\nTrial 2: Value=None, Params={'num_layers': 2, 'hidden_size': 1042, 'dropout_rate': 0.13704946756440722, 'learning_rate': 0.0001962824540572799, 'batch_size': 512}\nTrial 3: Value=0.0058866909191924695, Params={'num_layers': 2, 'hidden_size': 1021, 'dropout_rate': 0.19712927077787873, 'learning_rate': 0.0005317541785270053, 'batch_size': 1024}\nTrial 4: Value=0.0011420252040246516, Params={'num_layers': 4, 'hidden_size': 1431, 'dropout_rate': 0.14874841232122568, 'learning_rate': 0.0001668154090693652, 'batch_size': 256}\nTrial 5: Value=0.001180345466959157, Params={'num_layers': 2, 'hidden_size': 563, 'dropout_rate': 0.14888228757028477, 'learning_rate': 0.0005164858328527937, 'batch_size': 1024}\nTrial 6: Value=0.001968251105443791, Params={'num_layers': 5, 'hidden_size': 535, 'dropout_rate': 0.0698260696243658, 'learning_rate': 0.000286572814277565, 'batch_size': 256}\nTrial 7: Value=0.0002476581408997418, Params={'num_layers': 5, 'hidden_size': 770, 'dropout_rate': 0.058418252474786825, 'learning_rate': 0.00017537517229441845, 'batch_size': 1024}\nTrial 8: Value=0.0007346304221439878, Params={'num_layers': 3, 'hidden_size': 1366, 'dropout_rate': 0.058899333349805974, 'learning_rate': 0.0007806759254000885, 'batch_size': 2048}\nTrial 9: Value=0.04378657635985589, Params={'num_layers': 5, 'hidden_size': 1914, 'dropout_rate': 0.165165549528068, 'learning_rate': 0.000923670542014514, 'batch_size': 2048}\nTrial 10: Value=0.0007040701887545969, Params={'num_layers': 3, 'hidden_size': 1040, 'dropout_rate': 0.053985803299763525, 'learning_rate': 0.0004969399249798343, 'batch_size': 512}\nTrial 11: Value=0.00048549690792261033, Params={'num_layers': 5, 'hidden_size': 1016, 'dropout_rate': 0.007136966604876594, 'learning_rate': 0.0006812483863391891, 'batch_size': 2048}\nTrial 12: Value=None, Params={'num_layers': 3, 'hidden_size': 1041, 'dropout_rate': 0.11397939659547732, 'learning_rate': 0.0002995625767172919, 'batch_size': 512}\n","output_type":"stream"}],"execution_count":2}]}